{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QAG_GRU_Stable.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhayalekal74/smart-question-generation/blob/master/QAG_GRU_Stable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF5oYYFnGoc9",
        "colab_type": "text"
      },
      "source": [
        "# Download GloVe Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfKe-8DoSHX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_version = \"glove.6B\"\n",
        "glove_version_specific = \"glove.6B.300d\"\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K1m__tdGtLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "url = \"http://nlp.stanford.edu/data/{}.zip\".format(glove_version)\n",
        "r = requests.get(url, stream=True)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MYbj4w-J_L4",
        "colab_type": "text"
      },
      "source": [
        "# Download SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PmzDprkIAoN",
        "colab_type": "code",
        "outputId": "f41ae9f6-d1fd-4290-e524-8f146a7e093f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
        "dev_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
        "\n",
        "r = requests.get(train_url)\n",
        "open('train.json', 'wb').write(r.content)\n",
        "r = requests.get(dev_url)\n",
        "open('dev.json', 'wb').write(r.content)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4370528"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7V5jytZN1fg",
        "colab_type": "text"
      },
      "source": [
        "# Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMq5Vdq4Obtr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_csq(para_data):\n",
        "    contexts, sentences, questions, questions_output = list(), list(), list(), list()\n",
        "    for p in para_data:\n",
        "        for c in p.contexts:\n",
        "            for q in c.questions:\n",
        "                questions.append(q.text)\n",
        "                questions_output.append(q.output_question_text)\n",
        "                sentences.append(q.sentence)\n",
        "                contexts.append(c)\n",
        "    return contexts, sentences, questions, questions_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfQ8Z2pcrWfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import json\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "\n",
        "class Paragraph:\n",
        "    def __init__(self):\n",
        "        self.contexts = list()\n",
        "\n",
        "\n",
        "    def add_context(self, c):\n",
        "        self.contexts.append(c)\n",
        "\n",
        "\n",
        "class Context:\n",
        "    def __init__(self, c):\n",
        "        self.context = c\n",
        "        self.questions = list()\n",
        "\n",
        "\n",
        "    def add_question(self, q):\n",
        "        self.questions.append(q)\n",
        "\n",
        "\n",
        "class Question:\n",
        "    def __init__(self, q, s, a):\n",
        "        self.text = q\n",
        "        self.output_question_text = \" \".join(q.split()[1:])\n",
        "        self.is_impossible = False\n",
        "        self.sentence = s\n",
        "        self.answers = a\n",
        "\n",
        "def extract_data(file_name):\n",
        "    all_para_data = list()\n",
        "    with open(file_name, 'r') as f:\n",
        "        paragraphs = json.load(f)['data']\n",
        "    for p in paragraphs:\n",
        "        para_obj = Paragraph()\n",
        "        for c in p['paragraphs']:\n",
        "            context_text = c['context']\n",
        "            context_obj = Context(context_text)\n",
        "            for qa in c['qas']:\n",
        "                answers = list()\n",
        "                sentence = ''\n",
        "                for a in qa['answers']:\n",
        "                    ans_text = a['text']\n",
        "                    if not sentence:\n",
        "                        offset = a['answer_start']\n",
        "                        try:\n",
        "                            sen_start_index = context_text[:offset].rindex('.')\n",
        "                        except:\n",
        "                            sen_start_index = 0 \n",
        "                        ans_end_index = offset + len(ans_text)\n",
        "                        try:\n",
        "                            sen_end_index = context_text[ans_end_index:].index('.')\n",
        "                        except:\n",
        "                            sen_end_index = len(context_text)\n",
        "                        sentence = context_text[sen_start_index: sen_end_index + 1]\n",
        "                    answers.append(ans_text)\n",
        "                ques_obj = Question(qa['question'], sentence, answers)\n",
        "                context_obj.add_question(ques_obj)\n",
        "            para_obj.add_context(context_obj)\n",
        "        all_para_data.append(para_obj)\n",
        "    return all_para_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YwlozYlK6rT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = extract_data('train.json')\n",
        "contexts, sentences, questions, questions_output = get_csq(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edomOzPGPgv9",
        "colab_type": "text"
      },
      "source": [
        "# Create Embedding Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5IfI1vYPXIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings = dict()\n",
        "\n",
        "embeddings_file = '{}.txt'.format(glove_version_specific)\n",
        "\n",
        "with open(embeddings_file, 'r') as f:\n",
        "    for l in f.readlines():\n",
        "        words = l.split()\n",
        "        embeddings[words[0]] = np.asarray(words[1:], dtype=np.float32)\n",
        "    embeddings['<unk>'] = np.random.rand(EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyPRYbEVlQDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding(token):\n",
        "    try:\n",
        "        return embeddings[token]\n",
        "    except KeyError:\n",
        "        return embeddings['<unk>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lJC5HeYsgeJ",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHziUKgpQm_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = re.sub('(^[^a-z0-9]+(?=[a-z0-9]))|((?<=[a-z0-9])[^a-z0-9]+$)', '',  tokens[i])\n",
        "    return len(tokens), \" \".join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ40PyHjBUEI",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfvbNuALBQdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def fit_tokenizer(texts, max_most_frequent_words):\n",
        "    tokenizer = Tokenizer(num_words=max_most_frequent_words, filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_padded_sequences_from_texts(texts, tokenizer, max_seq_length):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(sequences, maxlen=max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99X27e_pG1oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get preprocessed contexts, sentences and questions\n",
        "# Get the max length of sequences in all three\n",
        "\n",
        "def get_preprocessed_data_and_max_seq_len(data):\n",
        "    preprocessed_data = [preprocess(x) for x in data]\n",
        "    max_len = max(d[0] for d in preprocessed_data)\n",
        "    return max_len, [d[1] for d in preprocessed_data]\n",
        "\n",
        "\n",
        "#TODO remove\n",
        "TRIM_LENGTH = 500\n",
        "\n",
        "context_max_len, contexts_preprocessed = get_preprocessed_data_and_max_seq_len([c.context for c in contexts[:TRIM_LENGTH]])\n",
        "sentence_max_len, sentences_preprocessed = get_preprocessed_data_and_max_seq_len(sentences[:TRIM_LENGTH]) \n",
        "question_max_len, questions_preprocessed = get_preprocessed_data_and_max_seq_len(questions[:TRIM_LENGTH])\n",
        "_, questions_output_preprocessed = get_preprocessed_data_and_max_seq_len(questions_output[:TRIM_LENGTH])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nrl-oAkR4qp",
        "colab_type": "code",
        "outputId": "e11876d5-5629-405a-b168-304b432e5938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (context_max_len, sentence_max_len, question_max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326 77 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSxLSFRLDnls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Use only the most frequent MAX_SRC_UNIQ_WORDS from src (contexts, sentences) data\n",
        "# Use only the most frequent MAX_TAR_UNIQ_WORDS from tar (questions) data\n",
        "\n",
        "MAX_SRC_UNIQ_WORDS = 10000\n",
        "MAX_TAR_UNIQ_WORDS = 7000\n",
        "\n",
        "# Fit two different tokenizers on src and tar data\n",
        "src_tokenizer = fit_tokenizer([c.context for c in contexts], MAX_SRC_UNIQ_WORDS)\n",
        "tar_tokenizer = fit_tokenizer(questions, MAX_TAR_UNIQ_WORDS)\n",
        "\n",
        "MAX_SRC_UNIQ_WORDS = min(MAX_SRC_UNIQ_WORDS, len(src_tokenizer.word_index) + 1)\n",
        "MAX_TAR_UNIQ_WORDS = min(MAX_TAR_UNIQ_WORDS, len(tar_tokenizer.word_index) + 1)\n",
        "\n",
        "\n",
        "# Pad the sequences to max_seq_length in corresponding data\n",
        "# context_sequences = get_padded_sequences_from_texts(contexts_preprocessed, src_tokenizer, context_max_len)\n",
        "# sentence_sequences = get_padded_sequences_from_texts(sentences_preprocessed, src_tokenizer, sentence_max_len)\n",
        "# question_input_sequences = get_padded_sequences_from_texts(questions_preprocessed, tar_tokenizer, question_max_len)\n",
        "# question_output_sequences = get_padded_sequences_from_texts(questions_output_preprocessed, tar_tokenizer, question_max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyH8W6TjLnnO",
        "colab_type": "text"
      },
      "source": [
        "# SRC and TAR embeddings matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gXLOvAK5jrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create embedding matrix for uniq words from src and tar tokenizer.word_index\n",
        "\n",
        "# def get_embedding_matrix(word_index, MAX_WORDS):\n",
        "#     max_uniq_words = min(MAX_WORDS, len(word_index) + 1)\n",
        "#     embeddings_matrix = np.zeros((max_uniq_words, EMBEDDING_DIM))\n",
        "#     for word, i in word_index.items():\n",
        "#         if i >= max_uniq_words:\n",
        "#             break\n",
        "#         word_embedding = get_embedding(word)\n",
        "#         embeddings_matrix[i] = word_embedding\n",
        "#     return embeddings_matrix\n",
        "\n",
        "def get_embedding_matrix(word_index, MAX_WORDS):\n",
        "    embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= MAX_WORDS:\n",
        "            continue\n",
        "        embedding_vector = get_embedding(word)\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X3FExW-9Dge",
        "colab_type": "code",
        "outputId": "fc06b59d-2064-4794-ff44-0132c038c18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "src_emb_matrix = get_embedding_matrix(src_tokenizer.word_index, MAX_SRC_UNIQ_WORDS)\n",
        "tar_emb_matrix = get_embedding_matrix(tar_tokenizer.word_index, MAX_TAR_UNIQ_WORDS)\n",
        "\n",
        "print (src_emb_matrix.shape, tar_emb_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 300) (7000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZe-LJSyV3gO",
        "colab_type": "text"
      },
      "source": [
        "# Without Embedding Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoQHp4A5cfqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXTRA_WORDS = 3\n",
        "\n",
        "def get_word_index_map(word_index, MAX_WORDS):\n",
        "    word_index_map, index_word_map = dict(), dict()\n",
        "    for word, i in word_index.items():\n",
        "        if i >= MAX_WORDS:\n",
        "            continue\n",
        "        else:\n",
        "            word_index_map[word] = i\n",
        "            index_word_map[i] = word\n",
        "    word_index_map['<unk>'] = MAX_WORDS\n",
        "    word_index_map['<SOS>'] = MAX_WORDS + 1\n",
        "    word_index_map['<EOS>'] = MAX_WORDS + 2\n",
        "    index_word_map[MAX_WORDS] = '<unk>'\n",
        "    index_word_map[MAX_WORDS + 1] = '<SOS>'\n",
        "    index_word_map[MAX_WORDS + 2] = '<EOS>'\n",
        "    return word_index_map, index_word_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc0D4aL3dCMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_word_index_map, src_index_word_map = get_word_index_map(src_tokenizer.word_index, MAX_SRC_UNIQ_WORDS)\n",
        "tar_word_index_map, tar_index_word_map = get_word_index_map(tar_tokenizer.word_index, MAX_TAR_UNIQ_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1NyWo3Hdxpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_index_of_token(word_index_map, token):\n",
        "    try:\n",
        "        return word_index_map[token]\n",
        "    except KeyError:\n",
        "        return word_index_map['<unk>']\n",
        "\n",
        "def get_token_at_index(index_word_map, index):\n",
        "    try:\n",
        "        return index_word_map[index]\n",
        "    except KeyError:\n",
        "        return index_word_map[1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HrLzDawb1P2",
        "colab_type": "text"
      },
      "source": [
        "**Preparing Input Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qrkAoc4VLSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Allocate arrays for inputs\n",
        "\n",
        "EXTRA_WORDS_ADDED_TO_SENTENCES = 2\n",
        "\n",
        "ctx_encoder_input_data = np.zeros(\n",
        "    (len(contexts_preprocessed), context_max_len, MAX_SRC_UNIQ_WORDS + EXTRA_WORDS),\n",
        "    dtype='float32')\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(sentences_preprocessed), sentence_max_len + EXTRA_WORDS_ADDED_TO_SENTENCES, MAX_SRC_UNIQ_WORDS + EXTRA_WORDS),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(questions_preprocessed), question_max_len + EXTRA_WORDS_ADDED_TO_SENTENCES, MAX_TAR_UNIQ_WORDS + EXTRA_WORDS),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(questions_preprocessed), question_max_len + EXTRA_WORDS_ADDED_TO_SENTENCES, MAX_TAR_UNIQ_WORDS + EXTRA_WORDS),\n",
        "    dtype='float32')\n",
        "\n",
        "for i in range(len(sentences_preprocessed)):\n",
        "    context = contexts_preprocessed[i]\n",
        "    sentence = \"{} {} {}\".format('<SOS>', sentences_preprocessed[i], '<EOS>')\n",
        "    question = \"{} {} {}\".format('<SOS>', questions_preprocessed[i], '<EOS>')\n",
        "    for t, token in enumerate(context.split()):\n",
        "        ctx_encoder_input_data[i, t, get_index_of_token(src_word_index_map, token)] = 1.\n",
        "    for t, token in enumerate(sentence.split()):\n",
        "        encoder_input_data[i, t, get_index_of_token(src_word_index_map, token)] = 1.\n",
        "    for t, token in enumerate(question.split()):\n",
        "        decoder_input_data[i, t, get_index_of_token(tar_word_index_map, token)] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, get_index_of_token(tar_word_index_map, token)] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cIdzcF4b6ln",
        "colab_type": "text"
      },
      "source": [
        "**Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd1E7JZXtfOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import LSTM, LSTMCell, RNN, Dense, Input, Bidirectional, Dropout, Add, Concatenate, TimeDistributed\n",
        "from keras.layers import Dropout\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuB361_cbVzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "LSTM_CELLS = 600\n",
        "\n",
        "# ctx_encoder_inputs = Input(shape=(None, MAX_SRC_UNIQ_WORDS + EXTRA_WORDS), name='ctx_encoder_input_1')\n",
        "# # ctx_enc_1 = Bidirectional(LSTM(LSTM_CELLS, name='ctx_LSTM_1', return_sequences=True))\n",
        "# ctx_enc_1 = LSTM(LSTM_CELLS, name='ctx_LSTM_1', return_sequences=True)\n",
        "# ctx_out_1 = ctx_enc_1(ctx_encoder_inputs)\n",
        "# ctx_drop_1 = Dropout(0.3, name='ctx_Drop_1')(ctx_out_1)\n",
        "# # _, ctx_forward_h, ctx_forward_c, ctx_backward_h, ctx_backward_c = Bidirectional(LSTM(LSTM_CELLS, return_state=True, return_sequences=True, name='ctx_LSTM_2'))(ctx_drop_1)\n",
        "# _, ctx_state_h, ctx_state_c = LSTM(LSTM_CELLS, return_state=True, return_sequences=True, name='ctx_LSTM_2')(ctx_drop_1)\n",
        "# # ctx_enc_states = [ctx_state_h, ctx_state_c]\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, MAX_SRC_UNIQ_WORDS + EXTRA_WORDS), name='encoder_input_1')\n",
        "# encoder_1 = Bidirectional(LSTM(LSTM_CELLS, name='enc_LSTM_1', return_sequences=True), name='enc_Bidirectional_1')\n",
        "encoder_1 = GRU(LSTM_CELLS, name='enc_LSTM_1', return_sequences=True)\n",
        "encoder_1_outputs = encoder_1(encoder_inputs)\n",
        "enc_dropout_1 = Dropout(0.3, name='enc_Dropout_1')(encoder_1_outputs)\n",
        "# encoder_output, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(LSTM_CELLS, return_state=True, return_sequences=True, name='enc_LSTM_2'), name='enc_Bidirectional_2')(encoder_dropout_1)\n",
        "encoder_output, encoder_state = GRU(LSTM_CELLS, return_state=True, return_sequences=True, name='enc_LSTM_2')(enc_dropout_1)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "# ctx_h = Add()([ctx_forward_h, ctx_backward_h])\n",
        "# ctx_c = Add()([ctx_forward_c, ctx_backward_c])\n",
        "\n",
        "# h = Add()([forward_h, backward_h])\n",
        "# c = Add()([forward_c, backward_c])\n",
        "\n",
        "# state_h = Concatenate()([ctx_h, h])\n",
        "# state_c = Concatenate()([ctx_c, c])\n",
        "# state_h = Concatenate()([ctx_state_h, state_h])\n",
        "# state_c = Concatenate()([ctx_state_c, state_c])\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, MAX_TAR_UNIQ_WORDS + EXTRA_WORDS), name='decoder_input_1')\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm_1 = GRU(LSTM_CELLS, return_sequences=True, return_state=True, name='decoder_LSTM_1')\n",
        "# decoder_lstm_1.cell.setAttentionMode(True)\n",
        "decoder_1_outputs, _ = decoder_lstm_1(inputs=decoder_inputs, initial_state=encoder_state)\n",
        "\n",
        "# attn_out, attn_states = AttentionLayer(name='attention_layer')([encoder_output, decoder_1_outputs])\n",
        "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_1_outputs, attn_out])\n",
        "\n",
        "decoder_dense = TimeDistributed(Dense(MAX_TAR_UNIQ_WORDS + EXTRA_WORDS, activation='softmax', name='decoder_Dense_1'))\n",
        "decoder_outputs = decoder_dense(decoder_1_outputs)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "# model = Model([ctx_encoder_inputs, encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "# model.fit([ctx_encoder_input_data, encoder_input_data, decoder_input_data], decoder_target_data, batch_size=32, epochs=50)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=10, callbacks=[es])\n",
        "# Save model\n",
        "model.save('qag.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUDk5BQmcA3A",
        "colab_type": "text"
      },
      "source": [
        "**Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8FlCncacEGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_model = None\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_state)\n",
        "\n",
        "decoder_state_input = Input(shape=(LSTM_CELLS,))\n",
        "decoder_outputs, decoder_state = decoder_lstm_1(\n",
        "    decoder_inputs, initial_state=decoder_state_input)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs, decoder_state_input],\n",
        "    [decoder_outputs, decoder_state])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaLV5Orde91g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_question(ctx_input, input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict([input_seq])\n",
        "\n",
        "    # Generate empty target sequence of length 1 word.\n",
        "    target_seq = np.zeros((1, 1, MAX_TAR_UNIQ_WORDS + EXTRA_WORDS))\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0, get_index_of_token(tar_word_index_map, '<SOS>')] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    question_generated = list()\n",
        "    question_generated.append('<SOS>')\n",
        "    while not stop_condition:\n",
        "        output_tokens, state = decoder_model.predict(\n",
        "            [target_seq, states_value])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = get_token_at_index(tar_index_word_map, sampled_token_index)\n",
        "        question_generated.append(sampled_token)\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_token == '<EOS>' or len(question_generated) > question_max_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence.\n",
        "        target_seq = np.zeros((1, 1, MAX_TAR_UNIQ_WORDS + EXTRA_WORDS))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = state\n",
        "\n",
        "    return \" \".join(question_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDTRvAUQobxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (ctx_encoder_input_data[0:1].shape, encoder_input_data[0:1].shape)\n",
        "ctx_input = np.random.rand(1,326,10003)\n",
        "input_seq = np.random.rand(1,79,10003)\n",
        "question_generated = generate_question(ctx_input, input_seq)\n",
        "print('-')\n",
        "print('Input sentence:', input_seq)\n",
        "print('Decoded sentence:', question_generated)\n",
        "\n",
        "\n",
        "\n",
        "# ctx_input = ctx_encoder_input_data[1:2]\n",
        "# input_seq = encoder_input_data[1:2]\n",
        "# question_generated = generate_question(ctx_input, input_seq)\n",
        "# print('-')\n",
        "# print('Input sentence:', input_seq)\n",
        "# print('Decoded sentence:', question_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxOkpbIXqEW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(context, sentences):\n",
        "    context = preprocess(context)\n",
        "    test_sen_preprocessed = [preprocess(x) for x in sentences]\n",
        "    ctx_input = np.zeros((\n",
        "            1, context_max_len, MAX_SRC_UNIQ_WORDS + EXTRA_WORDS\n",
        "        ))\n",
        "    for i in range(len(test_sen_preprocessed)):\n",
        "        input_data = np.zeros((\n",
        "            1, sentence_max_len + EXTRA_WORDS_ADDED_TO_SENTENCES, MAX_SRC_UNIQ_WORDS + EXTRA_WORDS\n",
        "        ))\n",
        "        sentence = \"{} {} {}\".format(\"<SOS>\", test_sen_preprocessed[i][1], \"<EOS>\")\n",
        "        print (\"Input Sentence:\", sentence)\n",
        "        for t, token in enumerate(sentence.split()):\n",
        "            input_data[0, t, get_index_of_token(src_word_index_map, token)] = 1.\n",
        "        # ctx_input = np.random.rand(1,326,10003)\n",
        "        question_generated = generate_question(ctx_input, input_data)\n",
        "        print (\"Question Generated:\", question_generated)\n",
        "        print ()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx6ORClK_DNk",
        "colab_type": "code",
        "outputId": "a85b33ce-630d-4bd0-e1e8-a5f6ad315ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Model (stable) Output before change\n",
        "context = \"Beyoncé Giselle Knowles-Carter is an American singer, songwriter and actress. Born and raised in Houston, Texas, Beyoncé performed in various singing and dancing competitions as a child. She rose to fame in the late 1990s as lead singer of the R&B girl-group Destiny's Child, one of the best-selling girl groups in history. Their hiatus saw the release of her first solo album, Dangerously in Love (2003), which debuted at number one on the US Billboard 200 chart and earned her five Grammy Awards. The album also featured the US Billboard Hot 100 number-one singles Crazy in Love and Baby Boy. Following the break-up of Destiny's Child in 2006, she released her second solo album, B'Day, which contained her fourth solo number-one song in the US, Irreplaceable. Beyoncé also continued her acting career with starring roles in The Pink Panther (2006), Dreamgirls (2006), and Obsessed (2009). Her marriage to rapper Jay-Z and her portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the introduction of her alter-ego, Sasha Fierce, and earned a record-setting six Grammy Awards in 2010, including Song of the Year for Single Ladies (Put a Ring on It). After splitting from her manager and father Mathew Knowles in 2010, she released 4 in 2011, which explored a mellower tone and was influenced by 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed eponymous album, released in 2013 with no prior announcement, was even more experimental in its production and exploration of darker themes. Lemonade (2016), one of her most personal and political work to date, received widespread critical acclaim and became the best-selling album of 2016. In 2018, she released Everything Is Love, a collaborative album with her husband, Jay-Z, as The Carters. Throughout her career, Beyoncé has sold over 100 million records worldwide as a solo artist and a further 60 million records with Destiny's Child, With the release of Lemonade, Beyoncé became the first and only musical act in Billboard chart history to debut at number one with their first six solo studio albums\"\n",
        "sentences = [\n",
        "             \"Beyoncé Giselle Knowles-Carter is an American singer, songwriter and actress. Born and raised in Houston, Texas, Beyoncé performed in various singing and dancing competitions as a child\",\n",
        "             \"She rose to fame in the late 1990s as lead singer of the R&B girl-group Destiny's Child, one of the best-selling girl groups in history\",\n",
        "             \"Their hiatus saw the release of her first solo album, Dangerously in Love (2003), which debuted at number one on the US Billboard 200 chart and earned her five Grammy Awards. The album also featured the US Billboard Hot 100 number-one singles Crazy in Love and Baby Boy\",\n",
        "             \"Following the break-up of Destiny's Child in 2006, she released her second solo album, B'Day, which contained her fourth solo number-one song in the US, Irreplaceable\",\n",
        "             \"Beyoncé also continued her acting career with starring roles in The Pink Panther (2006), Dreamgirls (2006), and Obsessed (2009)\",\n",
        "             \"Her marriage to rapper Jay-Z and her portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the introduction of her alter-ego, Sasha Fierce, and earned a record-setting six Grammy Awards in 2010, including Song of the Year for Put a Ring on It\",\n",
        "             \"After splitting from her manager and father Mathew Knowles in 2010, she released 4 in 2011, which explored a mellower tone and was influenced by 1970s funk, 1980s pop, and 1990s soul\",\n",
        "             \"Her critically acclaimed eponymous album, released in 2013 with no prior announcement, was even more experimental in its production and exploration of darker themes\",\n",
        "             \"Lemonade (2016), one of her most personal and political work to date, received widespread critical acclaim and became the best-selling album of 2016. In 2018, she released Everything Is Love, a collaborative album with her husband, Jay-Z, as The Carters\"\n",
        "]\n",
        "\n",
        "test(context, sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sentence: <SOS> beyonc giselle knowles-carter is an american singer songwriter and actress born and raised in houston texas beyonc performed in various singing and dancing competitions as a child <EOS>\n",
            "Question Generated: <SOS> what did beyoncé's mother own when <unk> was a child <EOS>\n",
            "\n",
            "Input Sentence: <SOS> she rose to fame in the late 1990s as lead singer of the r&b girl-group destiny's child one of the best-selling girl groups in history <EOS>\n",
            "Question Generated: <SOS> what was the name of beyoncé's first solo album <EOS>\n",
            "\n",
            "Input Sentence: <SOS> their hiatus saw the release of her first solo album dangerously in love 2003 which debuted at number one on the us billboard 200 chart and earned her five grammy awards the album also featured the us billboard hot 100 number-one singles crazy in love and baby boy <EOS>\n",
            "Question Generated: <SOS> what did beyoncé's mother own when <unk> was a child <EOS>\n",
            "\n",
            "Input Sentence: <SOS> following the break-up of destiny's child in 2006 she released her second solo album b'day which contained her fourth solo number-one song in the us irreplaceable <EOS>\n",
            "Question Generated: <SOS> beyonce along with jay z met with <unk> family after their death <EOS>\n",
            "\n",
            "Input Sentence: <SOS> beyonc also continued her acting career with starring roles in the pink panther 2006 dreamgirls 2006 and obsessed 2009 <EOS>\n",
            "Question Generated: <SOS> beyonce's father worked as a sales manager for what company <EOS>\n",
            "\n",
            "Input Sentence: <SOS> her marriage to rapper jay-z and her portrayal of etta james in cadillac records 2008 influenced her third album i am sasha fierce 2008 which saw the introduction of her alter-ego sasha fierce and earned a record-setting six grammy awards in 2010 including song of the year for put a ring on it <EOS>\n",
            "Question Generated: <SOS> what is the name of beyoncé's documentary film <EOS>\n",
            "\n",
            "Input Sentence: <SOS> after splitting from her manager and father mathew knowles in 2010 she released 4 in 2011 which explored a mellower tone and was influenced by 1970s funk 1980s pop and 1990s soul <EOS>\n",
            "Question Generated: <SOS> beyonce's father worked as a sales manager for what company <EOS>\n",
            "\n",
            "Input Sentence: <SOS> her critically acclaimed eponymous album released in 2013 with no prior announcement was even more experimental in its production and exploration of darker themes <EOS>\n",
            "Question Generated: <SOS> what movie did beyonce act in 2006 <EOS>\n",
            "\n",
            "Input Sentence: <SOS> lemonade 2016 one of her most personal and political work to date received widespread critical acclaim and became the best-selling album of 2016 in 2018 she released everything is love a collaborative album with her husband jay-z as the carters <EOS>\n",
            "Question Generated: <SOS> beyonce's mother worked in what industry <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHZZpNYF_JNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# New model\n",
        "\n",
        "sentences = [\n",
        "             \"Beyoncé Giselle Knowles-Carter is an American singer, songwriter and actress. Born and raised in Houston, Texas, Beyoncé performed in various singing and dancing competitions as a child\",\n",
        "             \"She rose to fame in the late 1990s as lead singer of the R&B girl-group Destiny's Child, one of the best-selling girl groups in history\",\n",
        "             \"Their hiatus saw the release of her first solo album, Dangerously in Love (2003), which debuted at number one on the US Billboard 200 chart and earned her five Grammy Awards. The album also featured the US Billboard Hot 100 number-one singles Crazy in Love and Baby Boy\",\n",
        "             \"Following the break-up of Destiny's Child in 2006, she released her second solo album, B'Day, which contained her fourth solo number-one song in the US, Irreplaceable\",\n",
        "             \"Beyoncé also continued her acting career with starring roles in The Pink Panther (2006), Dreamgirls (2006), and Obsessed (2009)\",\n",
        "             \"Her marriage to rapper Jay-Z and her portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the introduction of her alter-ego, Sasha Fierce, and earned a record-setting six Grammy Awards in 2010, including Song of the Year for Put a Ring on It\",\n",
        "             \"After splitting from her manager and father Mathew Knowles in 2010, she released 4 in 2011, which explored a mellower tone and was influenced by 1970s funk, 1980s pop, and 1990s soul\",\n",
        "             \"Her critically acclaimed eponymous album, released in 2013 with no prior announcement, was even more experimental in its production and exploration of darker themes\",\n",
        "             \"Lemonade (2016), one of her most personal and political work to date, received widespread critical acclaim and became the best-selling album of 2016. In 2018, she released Everything Is Love, a collaborative album with her husband, Jay-Z, as The Carters\"\n",
        "]\n",
        "\n",
        "test(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}